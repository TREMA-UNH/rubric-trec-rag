# RUBRIC relevance labels for TREC RAG 24 

More info on the RUBRIC evaluation metric from the Autograder Workbench resource: <https://github.com/TREMA-UNH/rubric-grading-workbench>

Research papers on the topic: <https://www.cs.unh.edu/~dietz/publications/index.html>

More info about the TREC RAG track: <https://trec-rag.github.io/annoucements/2024-track-guidelines/>

## Installation with Nix

Set up the NIX environment, including the RUBRIC code from the Autograder Workbench

    nix develop


Get the TREC RAG data: <https://trec-rag.github.io>

Place data in following locations

   * `./retrieval/`  submitted system runs for retrieval task
   * `topics.rag24.test.txt`  queries
   * `msmarco_v2.1_doc_segmented.tar` rag corpus (untar and index with duckdb)


## Usage

### Convert input data to RUBRIC format

1. Index TREC RAG corpus with duckdb 
```
    python -O -m rubric_trec_rag.build_rag_segment_index --out msmarco.duckdb  ./msmarco_v2.1_doc_segmented/msmarco_v2.1_doc_segmented_*.json.gz 
```
2. Convert retieval-run data to input files for RUBRIC
```
    python -O -m rubric_trec_rag.rag_retrieval_inputs --query-path topics.rag24.test.txt --query-out queries-rag24.json  -p rubric-rag-retrieval.jsonl.gz  --rag-corpus-db msmarco.duckdb   retrieval/*
```

3. Convert generation-run data to input files for RUBRIC

   (Todo)

### Rubric generation, grading, relevance label prediction

1. Run question bank generation from RUBRIC

    bash rubric-generation-rag24.sh



